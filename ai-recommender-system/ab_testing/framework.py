import asyncio
import random
import hashlib
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
from enum import Enum
import logging
from dataclasses import dataclass
from collections import defaultdict

from sqlalchemy.orm import Session
from database.database import get_db_context
from database.models import User, ImplicitFeedback, ExplicitFeedback
from models.base_recommender import RecommendationRequest, RecommendationResult
from models.content_based import ContentBasedRecommender
from models.collaborative_filtering import CollaborativeFilteringRecommender
from models.two_tower import TwoTowerRecommender
from models.hybrid_recommender import HybridRecommender
from analytics.metrics import RecommendationMetrics

logger = logging.getLogger(__name__)


class ExperimentStatus(Enum):
    DRAFT = "draft"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    CANCELLED = "cancelled"


@dataclass
class ExperimentConfig:
    """Configuration for an A/B test experiment."""
    name: str
    description: str
    algorithms: List[str]  # List of algorithm names to test
    traffic_split: Dict[str, float]  # Percentage of traffic for each algorithm
    target_user_types: List[str]  # User types to include in the test
    min_sample_size: int = 100  # Minimum users per variant
    max_duration_days: int = 30  # Maximum experiment duration
    significance_threshold: float = 0.05  # Statistical significance threshold
    primary_metric: str = "precision@5"  # Primary metric to optimize
    secondary_metrics: List[str] = None  # Additional metrics to track


@dataclass
class ExperimentResult:
    """Results from an A/B test experiment."""
    experiment_id: str
    algorithm: str
    users_assigned: int
    conversions: int
    conversion_rate: float
    metrics: Dict[str, float]
    confidence_interval: Tuple[float, float]
    statistical_significance: bool


class ABTestingFramework:
    """A/B testing framework for recommendation algorithms."""
    
    def __init__(self):
        self.experiments = {}
        self.user_assignments = {}  # user_id -> experiment_id -> algorithm
        self.metrics_calculator = RecommendationMetrics()
        
        # Initialize available algorithms
        self.available_algorithms = {
            "content_based": ContentBasedRecommender(),
            "collaborative": CollaborativeFilteringRecommender(),
            "two_tower": TwoTowerRecommender(),
            "hybrid": HybridRecommender()
        }
    
    def create_experiment(self, config: ExperimentConfig) -> str:
        """Create a new A/B test experiment."""
        try:
            # Validate configuration
            self._validate_config(config)
            
            # Generate experiment ID
            experiment_id = self._generate_experiment_id(config.name)\n            
            # Create experiment\n            experiment = {\n                "id": experiment_id,\n                "config": config,\n                "status": ExperimentStatus.DRAFT,\n                "created_at": datetime.utcnow(),\n                "started_at": None,\n                "ended_at": None,\n                "user_assignments": {},\n                "results": {},\n                "metrics_history": []\n            }\n            \n            self.experiments[experiment_id] = experiment\n            logger.info(f"Created experiment {experiment_id}: {config.name}")\n            \n            return experiment_id\n            \n        except Exception as e:\n            logger.error(f"Error creating experiment: {e}")\n            raise\n    \n    def start_experiment(self, experiment_id: str) -> bool:\n        """Start an A/B test experiment."""\n        try:\n            if experiment_id not in self.experiments:\n                raise ValueError(f"Experiment {experiment_id} not found")\n            \n            experiment = self.experiments[experiment_id]\n            \n            if experiment["status"] != ExperimentStatus.DRAFT:\n                raise ValueError(f"Cannot start experiment in status {experiment['status'].value}")\n            \n            # Update experiment status\n            experiment["status"] = ExperimentStatus.RUNNING\n            experiment["started_at"] = datetime.utcnow()\n            \n            logger.info(f"Started experiment {experiment_id}")\n            return True\n            \n        except Exception as e:\n            logger.error(f"Error starting experiment: {e}")\n            return False\n    \n    def stop_experiment(self, experiment_id: str) -> bool:\n        """Stop an A/B test experiment."""\n        try:\n            if experiment_id not in self.experiments:\n                raise ValueError(f"Experiment {experiment_id} not found")\n            \n            experiment = self.experiments[experiment_id]\n            \n            if experiment["status"] != ExperimentStatus.RUNNING:\n                raise ValueError(f"Cannot stop experiment in status {experiment['status'].value}")\n            \n            # Update experiment status\n            experiment["status"] = ExperimentStatus.COMPLETED\n            experiment["ended_at"] = datetime.utcnow()\n            \n            logger.info(f"Stopped experiment {experiment_id}")\n            return True\n            \n        except Exception as e:\n            logger.error(f"Error stopping experiment: {e}")\n            return False\n    \n    def assign_user_to_experiment(self, user_id: int, user_type: str, experiment_id: str) -> Optional[str]:\n        """Assign a user to an algorithm variant in an experiment."""\n        try:\n            if experiment_id not in self.experiments:\n                return None\n            \n            experiment = self.experiments[experiment_id]\n            config = experiment["config"]\n            \n            # Check if experiment is running\n            if experiment["status"] != ExperimentStatus.RUNNING:\n                return None\n            \n            # Check if user type is eligible\n            if user_type not in config.target_user_types:\n                return None\n            \n            # Check if user is already assigned\n            if user_id in experiment["user_assignments"]:\n                return experiment["user_assignments"][user_id]\n            \n            # Assign user to algorithm variant using deterministic hash\n            algorithm = self._assign_algorithm(user_id, config.traffic_split)\n            \n            # Store assignment\n            experiment["user_assignments"][user_id] = algorithm\n            \n            # Also store in global user assignments\n            if user_id not in self.user_assignments:\n                self.user_assignments[user_id] = {}\n            self.user_assignments[user_id][experiment_id] = algorithm\n            \n            logger.debug(f"Assigned user {user_id} to algorithm {algorithm} in experiment {experiment_id}")\n            return algorithm\n            \n        except Exception as e:\n            logger.error(f"Error assigning user to experiment: {e}")\n            return None\n    \n    async def get_recommendations_with_ab_testing(\n        self,\n        request: RecommendationRequest,\n        active_experiments: List[str] = None\n    ) -> Tuple[List[RecommendationResult], str]:\n        """Get recommendations with A/B testing applied."""\n        try:\n            user_id = request.user_id\n            user_type = request.user_type\n            \n            # Find active experiments for this user\n            if active_experiments is None:\n                active_experiments = [eid for eid, exp in self.experiments.items() \n                                    if exp["status"] == ExperimentStatus.RUNNING]\n            \n            # Try to assign user to an experiment\n            assigned_algorithm = None\n            experiment_used = None\n            \n            for exp_id in active_experiments:\n                algorithm = self.assign_user_to_experiment(user_id, user_type, exp_id)\n                if algorithm:\n                    assigned_algorithm = algorithm\n                    experiment_used = exp_id\n                    break\n            \n            # If no experiment assignment, use hybrid algorithm\n            if not assigned_algorithm:\n                assigned_algorithm = "hybrid"\n            \n            # Get recommendations from assigned algorithm\n            if assigned_algorithm in self.available_algorithms:\n                recommender = self.available_algorithms[assigned_algorithm]\n                \n                # Check if the specific algorithm can recommend for this user\n                if hasattr(recommender, 'can_recommend') and not recommender.can_recommend(user_id, user_type):\n                    # Fallback to hybrid if specific algorithm can't recommend\n                    recommender = self.available_algorithms["hybrid"]\n                    assigned_algorithm = "hybrid"\n                \n                recommendations = await recommender.recommend(request)\n            else:\n                # Fallback to hybrid\n                recommendations = await self.available_algorithms["hybrid"].recommend(request)\n                assigned_algorithm = "hybrid"\n            \n            # Log the assignment for analytics\n            if experiment_used:\n                logger.info(f"User {user_id} got recommendations from {assigned_algorithm} in experiment {experiment_used}")\n            \n            return recommendations, assigned_algorithm\n            \n        except Exception as e:\n            logger.error(f"Error getting recommendations with A/B testing: {e}")\n            # Fallback to hybrid in case of error\n            recommendations = await self.available_algorithms["hybrid"].recommend(request)\n            return recommendations, "hybrid"\n    \n    async def record_conversion(self, user_id: int, experiment_id: str, conversion_type: str = "engagement"):\n        """Record a conversion for A/B testing metrics."""\n        try:\n            if experiment_id not in self.experiments:\n                return\n            \n            experiment = self.experiments[experiment_id]\n            \n            # Check if user is assigned to this experiment\n            if user_id not in experiment["user_assignments"]:\n                return\n            \n            algorithm = experiment["user_assignments"][user_id]\n            \n            # Initialize results structure if needed\n            if "conversions" not in experiment["results"]:\n                experiment["results"]["conversions"] = defaultdict(int)\n            \n            # Record conversion\n            conversion_key = f"{algorithm}_{conversion_type}"\n            experiment["results"]["conversions"][conversion_key] += 1\n            \n            logger.debug(f"Recorded {conversion_type} conversion for user {user_id} in experiment {experiment_id} (algorithm: {algorithm})")\n            \n        except Exception as e:\n            logger.error(f"Error recording conversion: {e}")\n    \n    async def calculate_experiment_results(self, experiment_id: str) -> List[ExperimentResult]:\n        """Calculate results for an A/B test experiment."""\n        try:\n            if experiment_id not in self.experiments:\n                raise ValueError(f"Experiment {experiment_id} not found")\n            \n            experiment = self.experiments[experiment_id]\n            config = experiment["config"]\n            user_assignments = experiment["user_assignments"]\n            \n            results = []\n            \n            # Group users by assigned algorithm\n            algorithm_users = defaultdict(list)\n            for user_id, algorithm in user_assignments.items():\n                algorithm_users[algorithm].append(user_id)\n            \n            # Calculate metrics for each algorithm\n            for algorithm, assigned_users in algorithm_users.items():\n                if not assigned_users:\n                    continue\n                \n                # Get recommendations for all users in this variant\n                user_recommendations = {}\n                \n                for user_id in assigned_users:\n                    try:\n                        with get_db_context() as db:\n                            user = db.query(User).filter(User.user_id == user_id).first()\n                            if not user:\n                                continue\n                        \n                        # Get recommendations using the specific algorithm\n                        request = RecommendationRequest(\n                            user_id=user_id,\n                            user_type=user.user_type,\n                            limit=20\n                        )\n                        \n                        if algorithm in self.available_algorithms:\n                            recommender = self.available_algorithms[algorithm]\n                            recommendations = await recommender.recommend(request)\n                            if recommendations:\n                                user_recommendations[user_id] = recommendations\n                    \n                    except Exception as e:\n                        logger.warning(f"Error getting recommendations for user {user_id}: {e}")\n                        continue\n                \n                # Calculate metrics using the analytics framework\n                if user_recommendations:\n                    metrics = await self.metrics_calculator.evaluate_recommendations(user_recommendations)\n                    \n                    # Calculate conversions\n                    conversions = experiment["results"].get("conversions", {})\n                    conversion_key = f"{algorithm}_engagement"\n                    conversion_count = conversions.get(conversion_key, 0)\n                    conversion_rate = conversion_count / len(assigned_users) if assigned_users else 0.0\n                    \n                    # Calculate confidence interval (simplified)\n                    confidence_interval = self._calculate_confidence_interval(\n                        conversion_rate, len(assigned_users)\n                    )\n                    \n                    # Determine statistical significance (simplified)\n                    statistical_significance = (\n                        len(assigned_users) >= config.min_sample_size and\n                        confidence_interval[1] - confidence_interval[0] < 0.1  # Narrow CI\n                    )\n                    \n                    result = ExperimentResult(\n                        experiment_id=experiment_id,\n                        algorithm=algorithm,\n                        users_assigned=len(assigned_users),\n                        conversions=conversion_count,\n                        conversion_rate=conversion_rate,\n                        metrics={metric: data.get("mean", 0.0) for metric, data in metrics.items()},\n                        confidence_interval=confidence_interval,\n                        statistical_significance=statistical_significance\n                    )\n                    \n                    results.append(result)\n            \n            return results\n            \n        except Exception as e:\n            logger.error(f"Error calculating experiment results: {e}")\n            return []\n    \n    def get_experiment_status(self, experiment_id: str) -> Dict[str, Any]:\n        """Get current status and metrics for an experiment."""\n        try:\n            if experiment_id not in self.experiments:\n                raise ValueError(f"Experiment {experiment_id} not found")\n            \n            experiment = self.experiments[experiment_id]\n            config = experiment["config"]\n            \n            # Calculate runtime metrics\n            users_assigned = len(experiment["user_assignments"])\n            \n            # Group by algorithm\n            algorithm_counts = defaultdict(int)\n            for user_id, algorithm in experiment["user_assignments"].items():\n                algorithm_counts[algorithm] += 1\n            \n            # Calculate experiment duration\n            if experiment["started_at"]:\n                if experiment["ended_at"]:\n                    duration = (experiment["ended_at"] - experiment["started_at"]).days\n                else:\n                    duration = (datetime.utcnow() - experiment["started_at"]).days\n            else:\n                duration = 0\n            \n            return {\n                "experiment_id": experiment_id,\n                "name": config.name,\n                "status": experiment["status"].value,\n                "created_at": experiment["created_at"].isoformat(),\n                "started_at": experiment["started_at"].isoformat() if experiment["started_at"] else None,\n                "ended_at": experiment["ended_at"].isoformat() if experiment["ended_at"] else None,\n                "duration_days": duration,\n                "users_assigned": users_assigned,\n                "algorithm_distribution": dict(algorithm_counts),\n                "target_user_types": config.target_user_types,\n                "primary_metric": config.primary_metric,\n                "min_sample_size": config.min_sample_size,\n                "sample_size_reached": users_assigned >= config.min_sample_size\n            }\n            \n        except Exception as e:\n            logger.error(f"Error getting experiment status: {e}")\n            return {}\n    \n    def list_experiments(self) -> List[Dict[str, Any]]:\n        """List all experiments with their basic info."""\n        try:\n            experiments_list = []\n            \n            for exp_id, experiment in self.experiments.items():\n                config = experiment["config"]\n                \n                experiments_list.append({\n                    "experiment_id": exp_id,\n                    "name": config.name,\n                    "description": config.description,\n                    "status": experiment["status"].value,\n                    "algorithms": config.algorithms,\n                    "created_at": experiment["created_at"].isoformat(),\n                    "users_assigned": len(experiment["user_assignments"])\n                })\n            \n            return experiments_list\n            \n        except Exception as e:\n            logger.error(f"Error listing experiments: {e}")\n            return []\n    \n    def _validate_config(self, config: ExperimentConfig):\n        """Validate experiment configuration."""\n        # Check algorithms exist\n        for algorithm in config.algorithms:\n            if algorithm not in self.available_algorithms:\n                raise ValueError(f"Unknown algorithm: {algorithm}")\n        \n        # Check traffic split sums to 1.0\n        total_split = sum(config.traffic_split.values())\n        if abs(total_split - 1.0) > 0.001:\n            raise ValueError(f"Traffic split must sum to 1.0, got {total_split}")\n        \n        # Check traffic split matches algorithms\n        for algorithm in config.algorithms:\n            if algorithm not in config.traffic_split:\n                raise ValueError(f"Traffic split missing for algorithm: {algorithm}")\n        \n        # Check user types\n        valid_user_types = ["developer", "founder", "investor"]\n        for user_type in config.target_user_types:\n            if user_type not in valid_user_types:\n                raise ValueError(f"Invalid user type: {user_type}")\n        \n        # Set default secondary metrics if not provided\n        if config.secondary_metrics is None:\n            config.secondary_metrics = ["recall@5", "ndcg@5", "diversity"]\n    \n    def _generate_experiment_id(self, name: str) -> str:\n        """Generate a unique experiment ID."""\n        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")\n        name_hash = hashlib.md5(name.encode()).hexdigest()[:8]\n        return f"exp_{timestamp}_{name_hash}"\n    \n    def _assign_algorithm(self, user_id: int, traffic_split: Dict[str, float]) -> str:\n        """Assign user to algorithm using deterministic hashing."""\n        # Use hash of user ID for deterministic assignment\n        user_hash = hashlib.md5(str(user_id).encode()).hexdigest()\n        hash_value = int(user_hash[:8], 16) / 0xffffffff  # Normalize to [0, 1)\n        \n        # Assign based on traffic split\n        cumulative = 0.0\n        for algorithm, split in traffic_split.items():\n            cumulative += split\n            if hash_value < cumulative:\n                return algorithm\n        \n        # Fallback to first algorithm\n        return list(traffic_split.keys())[0]\n    \n    def _calculate_confidence_interval(self, conversion_rate: float, sample_size: int) -> Tuple[float, float]:\n        """Calculate 95% confidence interval for conversion rate."""\n        if sample_size == 0:\n            return (0.0, 0.0)\n        \n        # Using normal approximation for binomial proportion\n        import math\n        z_score = 1.96  # 95% confidence\n        \n        # Avoid division by zero\n        p = max(0.001, min(0.999, conversion_rate))\n        \n        margin_of_error = z_score * math.sqrt((p * (1 - p)) / sample_size)\n        \n        lower = max(0.0, p - margin_of_error)\n        upper = min(1.0, p + margin_of_error)\n        \n        return (lower, upper)